{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "                EMA_decay: 0.9999                        \n",
      "               batch_size: 8                             \t[default: 1]\n",
      "               channels_G: 64                            \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                ckpt_iter: best                          \n",
      "                 dataroot: data/ADEChallengeData2016/    \t[default: ./datasets/cityscapes/]\n",
      "             dataset_mode: ade20k                        \t[default: coco]\n",
      "                  gpu_ids: 0                             \n",
      "                     name: oasis_ade20k_pretrained       \t[default: label2coco]\n",
      "               no_3dnoise: False                         \n",
      "                   no_EMA: False                         \n",
      "                  no_flip: False                         \n",
      "         no_spectral_norm: False                         \n",
      "           num_res_blocks: 6                             \n",
      "              num_workers: 0                             \n",
      "          param_free_norm: syncbatch                     \n",
      "                    phase: test                          \t[default: train]\n",
      "              results_dir: ./results/                    \n",
      "                     seed: 42                            \n",
      "                 spade_ks: 3                             \n",
      "                    z_dim: 64                            \n",
      "----------------- End -------------------\n",
      "Created Ade20kDataset, size train: 2000, size val: 2000\n",
      "Created OASIS_Generator with 74314691 parameters\n",
      "Namespace(EMA_decay=0.9999, aspect_ratio=1.0, batch_size=8, cache_filelist_read=False, cache_filelist_write=False, channels_G=64, checkpoints_dir='./checkpoints', ckpt_iter='best', contain_dontcare_label=True, crop_size=256, dataroot='data/ADEChallengeData2016/', dataset_mode='ade20k', gpu_ids='0', label_nc=150, load_size=256, name='oasis_ade20k_pretrained', no_3dnoise=False, no_EMA=False, no_flip=False, no_spectral_norm=False, num_res_blocks=6, num_workers=0, param_free_norm='syncbatch', phase='test', results_dir='./results/', seed=42, semantic_nc=151, spade_ks=3, z_dim=64)\n",
      "<class 'argparse.Namespace'>\n",
      "/home/alexander/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:386: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "8 64 8 256 256\n",
      "/home/alexander/.local/lib/python3.8/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "8 64 8 256 256\n",
      "8 64 8 256 256\n",
      "8 64 8 256 256\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"test.py\", line 27, in <module>\n",
      "    image_saver(label, generated, data_i[\"name\"])\n",
      "  File \"/home/alexander/GAN/OASIS/utils/utils.py\", line 39, in __call__\n",
      "    im = tens_to_lab(label[i], self.num_cl)\n",
      "  File \"/home/alexander/GAN/OASIS/utils/utils.py\", line 224, in tens_to_lab\n",
      "    label_tensor = Colorize(tens, num_cl)\n",
      "  File \"/home/alexander/GAN/OASIS/utils/utils.py\", line 247, in Colorize\n",
      "    mask = (label == tens[0]).cpu()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 test.py --dataset_mode ade20k --name oasis_ade20k_pretrained --dataroot data/ADEChallengeData2016/ --batch 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import config\n",
    "import utils.utils as utils\n",
    "\n",
    "import models.models as models\n",
    "import dataloaders.dataloaders as dataloaders\n",
    "import utils.utils as utils\n",
    "import config\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "type(parser)\n",
    "parser = config.add_all_arguments(parser, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = parser.parse_args(\"--dataset_mode ade20k --name oasis_ade20k_pretrained --dataroot data/ADEChallengeData2016/ --batch 4 --num_workers 4\".split()) \n",
    "opt.phase = \"test\"\n",
    "opt.continue_train = False\n",
    "opt.add_vgg_loss = False\n",
    "utils.fix_seed(opt.seed)\n",
    "# --ckpt_iter='best' --contain_dontcare_label=True --crop_size=256 --dataroot='data/ADEChallengeData2016/', \\\n",
    "# dataset_mode='ade20k', gpu_ids='0', label_nc=150, load_size=256, name='oasis_ade20k_pretrained', no_3dnoise=False, \\\n",
    "# no_EMA=False, no_flip=False, no_spectral_norm=False, num_res_blocks=6, num_workers=0, \\\n",
    "# param_free_norm='syncbatch', phase='test', results_dir='./results/', seed=42, semantic_nc=151, spade_ks=3, z_dim=64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opt = \"EMA_decay=0.9999, aspect_ratio=1.0, batch_size=8, cache_filelist_read=False, cache_filelist_write=False, channels_G=64, checkpoints_dir='./checkpoints', \\\n",
    "ckpt_iter='best', contain_dontcare_label=True, crop_size=256, dataroot='data/ADEChallengeData2016/', \\\n",
    "dataset_mode='ade20k', gpu_ids='0', label_nc=150, load_size=256, name='oasis_ade20k_pretrained', no_3dnoise=False, \\\n",
    "no_EMA=False, no_flip=False, no_spectral_norm=False, num_res_blocks=6, num_workers=0, \\\n",
    "param_free_norm='syncbatch', phase='test', results_dir='./results/', seed=42, semantic_nc=151, spade_ks=3, z_dim=64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Ade20kDataset, size train: 2000, size val: 2000\n",
      "Created OASIS_Generator with 74314691 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:386: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/alexander/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:386: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/alexander/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:386: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/alexander/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:386: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/alexander/.local/lib/python3.8/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/alexander/GAN/OASIS/inference.ipynb Cell 2'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexander/GAN/OASIS/inference.ipynb#ch0000001?line=15'>16</a>\u001b[0m _, label \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mpreprocess_input(opt, data_i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexander/GAN/OASIS/inference.ipynb#ch0000001?line=16'>17</a>\u001b[0m generated \u001b[39m=\u001b[39m model(\u001b[39mNone\u001b[39;00m, label, \u001b[39m\"\u001b[39m\u001b[39mgenerate\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alexander/GAN/OASIS/inference.ipynb#ch0000001?line=17'>18</a>\u001b[0m image_saver(label, generated, data_i[\u001b[39m\"\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/GAN/OASIS/utils/utils.py:39\u001b[0m, in \u001b[0;36mresults_saver.__call__\u001b[0;34m(self, label, generated, name)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(label) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(generated)\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(label)):\n\u001b[0;32m---> 39\u001b[0m     im \u001b[39m=\u001b[39m tens_to_lab(label[i], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_cl)\n\u001b[1;32m     40\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_im(im, \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m, name[i])\n\u001b[1;32m     41\u001b[0m     im \u001b[39m=\u001b[39m tens_to_im(generated[i]) \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m\n",
      "File \u001b[0;32m~/GAN/OASIS/utils/utils.py:224\u001b[0m, in \u001b[0;36mtens_to_lab\u001b[0;34m(tens, num_cl)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtens_to_lab\u001b[39m(tens, num_cl):\n\u001b[0;32m--> 224\u001b[0m     label_tensor \u001b[39m=\u001b[39m Colorize(tens, num_cl)\n\u001b[1;32m    225\u001b[0m     label_numpy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(label_tensor\u001b[39m.\u001b[39mnumpy(), (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[1;32m    226\u001b[0m     \u001b[39mreturn\u001b[39;00m label_numpy\n",
      "File \u001b[0;32m~/GAN/OASIS/utils/utils.py:247\u001b[0m, in \u001b[0;36mColorize\u001b[0;34m(tens, num_cl)\u001b[0m\n\u001b[1;32m    244\u001b[0m tens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(tens, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    246\u001b[0m \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(cmap)):\n\u001b[0;32m--> 247\u001b[0m     mask \u001b[39m=\u001b[39m (label \u001b[39m==\u001b[39;49m tens[\u001b[39m0\u001b[39;49m])\u001b[39m.\u001b[39;49mcpu()\n\u001b[1;32m    248\u001b[0m     color_image[\u001b[39m0\u001b[39m][mask] \u001b[39m=\u001b[39m cmap[label][\u001b[39m0\u001b[39m]\n\u001b[1;32m    249\u001b[0m     color_image[\u001b[39m1\u001b[39m][mask] \u001b[39m=\u001b[39m cmap[label][\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#--- read options ---#\n",
    "# opt = config.read_arguments(train=False)\n",
    "\n",
    "#--- create dataloader ---#\n",
    "_, dataloader_val = dataloaders.get_dataloaders(opt)\n",
    "\n",
    "#--- create utils ---#\n",
    "image_saver = utils.results_saver(opt)\n",
    " \n",
    "#--- create models ---#\n",
    "model = models.OASIS_model(opt).cuda()\n",
    "#model = models.put_on_multi_gpus(model, opt)\n",
    "model.eval()\n",
    "#--- iterate over validation set ---#\n",
    "for i, data_i in enumerate(dataloader_val):\n",
    "    _, label = models.preprocess_input(opt, data_i)\n",
    "    generated = model(None, label, \"generate\", None)\n",
    "    image_saver(label, generated, data_i[\"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1748, device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_i['image'][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADE_val_00000021.jpg'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_i['name'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:386: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.0001, device='cuda:0')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.transforms as TR\n",
    "img = Image.open(\"data/ADEChallengeData2016/images/validation/ADE_val_00000014.jpg\").convert(\"RGB\")\n",
    "img=TR.functional.to_tensor(img)\n",
    "img = TR.functional.resize(img, (256, 256), Image.BICUBIC)\n",
    "img = TR.functional.normalize(img, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "img = img.cuda()\n",
    "img.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.netD = models.discriminators.OASIS_Discriminator(opt).cuda()\n",
    "model.netD.load_state_dict(torch.load(\"checkpoints/oasis_ade20k_pretrained/models/best_D.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/.local/lib/python3.8/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/alexander/.local/lib/python3.8/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/alexander/.local/lib/python3.8/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/alexander/.local/lib/python3.8/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/alexander/.local/lib/python3.8/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "i = 0\n",
    "cls = 151\n",
    "shift = 0\n",
    "seed = 0\n",
    "gt = new\n",
    "for j in range(5):\n",
    "\ttorch.manual_seed(seed)\n",
    "\t# z = torch.randn([1, 64, 256, 256], dtype=torch.float32, device=\"cuda:0\")\n",
    "\tz = torch.randn(1, 64, dtype=torch.float32, device=\"cuda:0\")\n",
    "\tz = z.view(1, 64, 1, 1)\n",
    "\tz = z.expand(1, 64, 256, 256)\n",
    "\tif cls==151:\n",
    "\t\tz = z + shift\n",
    "\telse:\n",
    "\t\tz = (z[0]+shift)*gt[i][cls] \n",
    "\t\tz = z.unsqueeze(0)\n",
    "\tgenerated = model(None, gt[i].unsqueeze(0), \"generate\", None,z)\n",
    "\n",
    "\tim = utils.tens_to_lab(gt[i], 150)\n",
    "\tim = utils.tens_to_im(generated[0]) * 255\n",
    "\timage = Image.fromarray(np.uint8(im))\n",
    "\timage.show()\n",
    "\t# image.save(f\"check/image{seed}.jpg\")\n",
    "\tseed+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.netD(img.unsqueeze(0))[0][1:].unsqueeze(0).cuda()\n",
    "# labels[labels>0.5] = 1\n",
    "# labels[labels<0.5] = 0\n",
    "# labels = labels/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_labels = labels.argmax(axis=1)\n",
    "new = torch.zeros([1,151,256,256]).cuda()\n",
    "# new[0,new_labels] = 1\n",
    "\n",
    "for i in range(256):\n",
    "\tfor j in range(256):\n",
    "\t\tnew[0,int(new_labels[0,i,j]),i,j] = 1\n",
    "new[0,...,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([151, 256, 256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_labels.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
